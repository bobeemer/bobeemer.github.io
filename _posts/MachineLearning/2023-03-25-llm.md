---

layout: post
title: 语言大模型LLM为什么可以work？
category: 架构
tags: MachineLearning
keywords: llm chatgpt gpt bert

---

## 简介

* TOC
{:toc}

[张宏江：大模型发展机会与挑战](https://mp.weixin.qq.com/s/pjUaRD0YV2qb6MXZ-oVstQ)
1. 当模型足够大，语料足够多的时候，涌现这件事情出现就不足为奇。这就好比把你甩到一个外语环境中，见得多听得多，根本不用专门学语法就可以学会语言，这就是语料和模型规模的重要性。看的句子多了，就懂得语法；见的世面多了，就懂得推理和逻辑。ChatGPT在认知能力上前进了一大步，通过强化学习与NLP（自然语言处理）相结合，通过人的反馈强化学习，基本解决了自然语言理解与生成问题，并且展现出人类无中生有的原创能力。
2. 人们对知识的表示和调用发生了根本性变化。从关系数据库（SQL），到互联网信息检索，科技史上每次知识表示与调用方式的跃迁，都会掀起一次巨大的技术变革。
3. 大模型作为基础平台支撑无数智能应用。大模型在内容创意生成、对话、语言或风格互译、搜索等方面的能力，将为各应用领域带来百花齐放。而大模型基础平台，在数据层、模型层、中间层、应用层，都蕴藏着巨大发展机遇。

[热议的“中国版ChatGPT”，如何理解其意义？](https://mp.weixin.qq.com/s/wvLj4eWCX8jdpCECLjIZmw)
1. 张亚勤：我觉得可以把GPT这个系列的生成式AI模型看作一个由大模型组成的AI操作系统，和PC上的Windows，以及移动的安卓、iOS基本具有相似的意义。一个新的操作系统出来是什么意思？下面的硬件、上面的应用都会被重构、重塑，形成一个新的生态。
2. 生成式对话产品会颠覆搜索引擎现有的商业模式，科技公司不得不自我革命。你也会这么认为吗？我觉得不是。要是你没有这个产品的话，别人会革你的命。我们在搜索的时候，其实是在找知识，那现在有了生成式技术，它确实提供了一种找到知识的新能力。

## 发展脉络

[ChatGPT成功背后的技术原因及其对生命科学领域的启发](https://mp.weixin.qq.com/s/snXaWHr0VYFNYirSHRAvRw)
1. 早在上个世纪五十年代，就有学者提出了人工智能（Artificial Intelligence）的概念，其目的是希望让计算机拥有人类智能（或部分人类智能）。这个领域经过很多年的发展，依然没有突破，直到2012年出现了深度学习技术。深度学习主要解决了模型表示能力的瓶颈。我们面对的建模问题，比如图像理解、语言翻译、语音识别、分子-蛋白结合构象预测等技术，都是非常复杂的非线性问题，在深度学习出现之前，模型表示能力很弱，无法对这些复杂问题进行精确表示。而**深度学习技术，可以通过模型的层次堆叠，理论上可以构建任意深度的模型，突破了模型表示能力的瓶颈**，从而在语音识别、计算机视觉、自然语言理解等领域取得了突破性进展。
2. 这个阶段的主要局限，是非常依赖于标注数据的数量。由于模型参数变多，想要求解这么多的模型参数，需要大量的训练数据作为约束。而**想获得大量的标注数据非常贵，到亿级别之后就很难再有提升，数据支撑的有效模型大小也受到限制**。
3. 2017年，一个重要的基础工作Transformer出现了。2019年，一个叫作BERT的工作脱颖而出，BERT采用了一个叫作自监督预训练的思路，无需标注数据**仅利用文本语料本身存在的约束就可以训练模型**（比如某句话的某个位置只能用某些限定的词），这样互联网上存在的优质语料不需要进行人工标定就可以用来做训练，从而一下子使得可用训练数据的数量有了巨大的提高，再配合上大模型，使得BERT模型的效果远远超过过去的模型，并且在不同任务间具有很好的通用性，成为NLP领域里程碑工作之一。其实在BERT出现之前的2018年，还有个工作叫作GPT（即GPT1.0），更早利用了自监督预训练的思路来做文本生成，即输入前面的文本，模型预测输出后面的文本，领域里面的优质语料无需标注就可以做训练。**BERT和GPT都是在Transformer基础上发展而来的，而Transformer也逐渐发展成为AI领域的通用模型**。
4. 在自监督预训练技术出现之后，我们可以认为新一代人工智能发展到了第二个阶段，即自监督预训练技术使得可用训练数据有了几个数量级的提升，**在训练数据大幅提升的支撑下，模型大小也有了数个数量级的提升（有效模型达到了千亿规模）**，而在模型效果上，这些模型变得不再依赖于下游任务领域数据的再训练，所以，**领域进入到基于自监督预训练的通用大模型时代**。
5. ChatGPT为什么能有这样惊艳的效果？其中一个核心原因是ChatGPT基于生成大模型GPT3.5构建，这应该是当前自然语言理解领域文本生成最好的模型（GPT3.5比GPT3.0使用了更多的数据和更大的模型，具有更好的效果）。第二个核心原因则是基于人类反馈的强化学习技术：第一步，先收集用户对于同一问题不同答案的偏好数据；第二步，利用这个偏好数据重新训练GPT模型，这一步是基于监督信息的精调；第三步，根据用户对于不同答案的偏好，训练一个打分函数，对于ChatGPT的答案会给出分数，这个分数会体现出用户对于不同答案的偏好；第四步，用这个打分函数作为强化学习的反馈（Reward）训练强化学习模型，使得ChatGPT最终输出的答案更偏向于用户喜欢的答案。通过上述过程，ChatGPT在GPT3.5的基础上，针对用户输入，输出对用户更友好的回答。ChatGPT第一阶段训练GPT生成模型使用的训练数据非常多，大约在几十TB，训练一次模型需要花费千万美元，而第二个阶段，基于强化学习的少量优质数据反馈则只需要数万条优质数据。这种新的范式，**有可能成为第三阶段人工智能的核心驱动技术，即首先基于自监督预训练的大模型，再结合基于少量优质数据反馈的强化学习、Prompting等技术，形成模型和数据的闭环反馈，获得进一步的技术突破**。如果这个技术走通，那么无人驾驶、机器人以及生命科学等数据获取昂贵的领域将显著受益。
6. ChatGPT并不能证明人工智能已经有了人类心智，ChatGPT表现出来的一些创造性和心智，是因为自然语言理解语料中包含了语义、逻辑，基于自然语言语料训练出来的生成模型，**统计意义上学习到了这些对应关系**，看起来似乎有了智能，但并不是真的有人类心智。ChatGPT很棒，但说他智力等于几岁小朋友的说法，都不够严谨。因为从根本上讲，人学习新知识、进行逻辑推理、想象、运动反馈这些能力，目前AI还没有具备。
7. ChatGPT并不是一两个研究人员做出的算法突破，而是在先进理念指导下，非常复杂的算法工程体系创造出来的成果，需要在团队和组织上匹配（类比OpenAI和DeepMind）。纯研究型的团队恐怕不能成功，对深度学习理解不够、太工程化的团队也不会成功。这只团队需要：第一要有足够资源支持，可以支撑昂贵的深度学习训练和人才招聘；第二要有真正在工业界领导过工程化大模型团队的专家领导，ChatGPT不仅有算法创新，更是工程体系创新；第三，也可能是最重要的，需要一个团结协作有统一领导且不追求论文发表的组织（松散型的组织利于算法创新，但不利于工程化算法攻坚），且配备足够多优秀的工程和算法人才。

脉络：由于自然语言任务种类繁多，且任务之间的差别不太大，所以为每个任务单独微调一份大模型很不划算 ==> pre-train model ==> 如何驾驭pre-train model?(Prompt Learning & Prompt Tuning）)

## 自然语言处理基础

1. 词表示。
    1. synonym and hypernym。用一个词相关的词来表示一个词
    2. one-hot。假定所有的文字一共有 N 个单词（也可以是字符），我们可以将每个单词赋予一个单独的序号 id，那么对于任意一个单词，我们都可以采用一个 N 位的列表（向量）对其进行表示。 缺点：这样会导致词汇与词汇之间是没有任何关联的。
    3. represent word by context
    4. word embedding。基于神经网络的词的向量表示方法。
2. 关键词的提取，关键词，顾名思义，就是能够表达文本中心内容的词语。
    1. 基于统计特征的方法
    2. 基于词图模型的关键词提取
    3. 基于主题模型的关键词提取
2. 语言模型。语言模型是根据语言客观事实而进行的语言抽象数学建模，是一种对应关系。根据前文预测下一个词是什么：计算一个序列的词成为一句话的概率是多少；根据已经出现的词，计算某个词出现的概率
    1. 统计语言模型。本质是基于词与词共现频次的统计。给定一个句子 `S=w1,w2,w3,…,wn`，则生成该句子的概率为：`p(S)=p(w1,w2,w3,w4,w5,…,wn)`，再由链式法则我们可以继续得到：`p(S)=p(w1)p(w2|w1)p(w3|w1,w2)…p(wn|w1,w2,…,wn-1)`。那么这个 p(S) 就是我们所要的统计语言模型。有一个非常本质的问题并没有被解决，那就是语料中数据必定存在稀疏的问题，公式中的很多部分是没有统计值的，那就成了 0 了，而且参数量真的实在是太大了。
        1. N-gram，前面出现的N个词，出现xx概率多大。但是如果 n 比较大，或者相关语料比较少的时候，数据稀疏问题仍然不能得到很好地解决。这就好比我们把水浒传的文本放入模型中进行统计训练，最后却问模型林冲和潘金莲的关系，这就很难回答了。因为基于 ngram 的统计模型实在是收集不到两者共现的文本。
    2. 神经网络语言模型。a neural language model is a language model based on netual networks to learn distributed representations of words. 给每个词分别赋予了向量空间的位置作为表征，从而计算它们在高维连续空间中的依赖关系。

有没有一种方法，可以**把语言变成一种数学计算过程**，比如采用概率、向量等方式对语言的生成和分析加以表示呢？之前很方案都是基于 符号、统计的，引入神经网络之后，一般用一个向量表示一个词，向量是学习出来的，并且可以根据新的语料学习调整。 

## 机器怎么看懂人类的文字？

[李宏毅-BERT and its family](https://www.bilibili.com/video/BV1eV411d7Kp)

![](/public/upload/machine/pre_train_fine_tune.jpg)

以往的NLP任务都是为每个任务分配一个模型，之后用**对应任务大量的标注数据**去训练这个模型。pre-train和fine-tune的流程是使用**大量的无标注文本**去预训练好一个模型，让这个模型可以初步的了解人类的语言，之后再使用对应任务的标注数据，去微调这个模型。这个过程和人类学习语言的过程是十分接近的。比如考托福考试，需要有听说读写这些测试。但是人类的学习的过程并不是直接去学习这些题目的做法，而是通过阅读大量的英文语料去掌握英文，之后再学习这个题目的解法，达到解题的效果。

what is pre-train model（也被称为foundation model）？ **represent each token by a embedding vector**.
1. contextualized word embedding。比如bank 单词有两种或三种意思，对于不同的意思的token，会根据其经常出现的上下文，学习一个 embedding token。 相关方法如ELMO等。
2. 不同于以往的embedding：输入一个token，输出一个embedding。contextualized word embedding是在输入一整个句子以后，输出这个句子中各个token的embedding。这样这个token的embedding就是在看过这个token的上下文后，输出token embedding，这个embedding就包含了上下文的信息。
2. contextualized word embedding的模型就像是一个encoder，用于编码信息。通常是非常深的网络，可以用到lstm，可以用自注意力层。
    ![](/public/upload/machine/pre_train_model.jpg)

how to fine-tune? fine-tune部分旨在根据预训练的model添加部分层从而可以解决下游任务。

![](/public/upload/machine/fine_tune.jpg)

**如何在预训练好的模型中再加入一部分让其可以实现下游任务？**现有的NLP任务的分类，其中按照输入可以分为两类，按照输出可以分为四类(one class;class for each token;copy from input;general sequence)。

输入可以分为两类
1. one sentence; 如句子分类
2. mutliple sentences； 比如QA，自然语言推理，需要在两个句子之间加入一个特殊的符号`[SEP]`（转为一个句子）。

输出可以分为四类
1. one class; BERT的解法是让一个特殊的符号`[CLS]`作为整个句子的表示，之后将`[CLS]`的embedding输入到一个分类器中，进行分类任务。也可以像其他模型一样，将所有token的embedding都输入到一个模型中（可以是各个token embedding的均值，也可以是各个token输入到RNN，再得到下一层的嵌入），进行分类。
2. class for each token; 如果任务是对每一个token进行分类的话，就需要将每一个token的embedding输入到一个网络中，对其进行分类。
3. copy from input。 比如说Extraction-based QA任务，其任务就是输入一段原文和一个问题，之后在原文中标注好哪个token是开始，哪个token是结尾。BERT论文中的解法就是设计了两个可以训练的embedding（一个是start，一个是end），之后用start和end向量分别和BERT得到的嵌入求内积，之后再通过softmax，计算哪个最大，就是最后的结果。
4. general sequence。生成任务
    1. BERT可以作为一个encoder，需要我们自己去设计一个decoder，但是decoder是没有经过预训练的。
    2. 让pre-training模型当作decoder来使用，其方法就是输入一个[sep]之后让model输出一个东西，再将模型的输出作为模型的输入，以此类推，不断的得到输出结果。

如何进行fine-tune呢？
1. fix住pre-train模型（feature extractor），只fine tune特定任务的模型。
2. 将特定任务模型和pre train的模型一起进行fine tune（也会改pre-train 部分的参数）。
    1. 问题是pre train的model是一样的，但是经过fine tune后，每个model是完全不同的，但是每一个model都是非常巨大的，NLP有很多的任务，如果每一个任务都要存储一个很大的模型，也许是行不通的。因此有了adaptor的概念。也就是说为每一个model存储一个adaptor（比如在原来的pre-train网络中加几层作为adapter，pre-train时不参与，fine-tune时进仅更新adapter。adapter插在pre-trian model 哪里有很大的的研究空间，花样很多，有专门的adapterhub），在fine-tune的时候，仅仅改变adaptor，并没有改变其他的信息（pre-train）。因此在存储模型的时候，只需要存储原来模型中的部分，以及每一个下游任务上的adaptor即可完成任务。所以这样的话，存储的参数量就少了很多。

how to pre-train?
1. 最早的跟预训练有关的模型，应该是CoVe，是一个基于翻译任务的一个模型，其用encoder的模块做预训练。但是CoVe需要大量的翻译对，这是不容易获得的，能不能通过一大段没有标注的语料进行预训练呢？
2. 比较直觉的self-supervised learning就是**预测下一个单词是什么**。给出的解法就是将一个token输入到网络中，经过softmax之后，得到下一个token的概率分布。

    ![](/public/upload/machine/pre_train.jpg)
1. 其中使用LSTM（上图的model = LSTM）做predict next token的工作有elmo，以及ulmfit。
2. 使用self-attention的方式（上图的model = LSTM）进行next token prediction。

有哪些预训练模型？

1. Left-to-Right LM: GPT, GPT-2, GPT-3。适合文字接龙
2. Masked LM: BERT, RoBERTa。适合文字填空
3. Prefix LM: UniLM1, UniLM2
4. Encoder-Decoder: T5, MASS, BART

## 对于大模型的两种不同的期待

[对于大模型的不同期待所衍生的两类使用方式](https://www.bilibili.com/video/BV1jA411274Q?p=9)

1. 成为专才。比如bert（做文字填空的，所以不能“说话”）
    1. 在进行具体的任务之前需要改造：加外挂（即为模型加额外模块）和微调参数（fine tune）。
    2. 有机会在专一任务有机会跑赢通才。
2. 成为通才。
    1. 需要额外进行prompt。 
    2. Multitask learning as Question Answering。所有自然语言的处理都是问答问题。
    2. 只要重新设计 prompt 就可以快速开发新功能，不用写程序。

比如做情感分析，你可以专门做一个情感分析的模型。也可以你给一点情感分析的例子，让模型去做文字接龙。

![](/public/upload/machine/in_context_learning.jpg)

再进一步，毕竟找范例有点麻烦，让机器根据叙述知道我们让它干嘛（instruction-tuning读懂人类的指令）。把任务用人类的语言描述出来，变成一个dataset，然后让llm 学到你让它干嘛，进而可以 generalized 到没有看过的指令上。

![](/public/upload/machine/instruction_tuning.jpg)


[一文详解Prompt学习和微调（Prompt Learning & Prompt Tuning）](https://zhuanlan.zhihu.com/p/620222350)

1. 过去许多机器学习方法是基于全监督学习（fully supervised learning）的。由于监督学习需要大量的数据学习性能优异的模型，而在 NLP 中大规模训练数据（指为特定任务而标注好的数据）是不足的，因此在深度学习出现之前研究者通常聚焦于**特征工程**（feature engineering），即利用领域知识从数据中提取好的特征；
2. 在深度学习出现之后, 由于特征可以从数据中习得，因此研究者转向了**结构工程**（architecture engineering），即通过通过设计一个合适的网络结构来把归纳偏置（inductive bias）引入模型中，从而有利于学习好的特征。
3. 在 2017-2019 年，NLP 模型开始转向一个新的模式（BERT），即预训练 + 微调（pre-train and fine-tune）。在这个模式中, 先用一个固定的结构预训练一个语言模型（language model, LM），预训练的方式就是让模型补全上下文（比如完形填空）。由于预训练不需要专家知识，因此可以在网络上搜集的大规模文本上直接进行训练。然后这个 LM 通过引入额外的参数或微调来适应到下游任务上。此时研究者转向了**目标工程**（objective engineering），即为预训练任务和微调任务设计更好的目标函数。
4. 在做 objective engineering 的过程中，研究者发现**让下游任务的目标与预训练的目标对齐是有好处的**（预训练是文字接龙，就把目标任务也改为文字接龙的形式，预训练是文字填空，就把目标任务也改为文字填空）。因此下游任务通过引入文本提示符（textual prompt），把原来的任务目标重构为与预训练模型一致的填空题。比如一个输入 “I missed the bus today.” 的重构：
    1. 情感预测任务。输入：“I missed the bus today.I felt so___.” 其中 “I felt so” 就是提示词（prompt），然后使用 LM 用一个表示情感的词填空。
    2. 翻译任务。输入：“English:I missed the bus today. French: ___.” 其中 “English:” 和 “French:” 就是提示词，然后使用 LM 应该再空位填入相应的法语句子。
5. 我们发现用不同的 prompt 加到相同的输入上，就能实现不同的任务，从而使得下游任务可以很好的对齐到预训练任务上，实现更好的预测效果。后来研究者发现，在同一个任务上使用不同的 prompt，预测效果也会有显著差异，因此现在有许多研究开始聚焦于 **prompt engineering**。

## chatgpt 是怎么炼成的

[ChatGPT (可能)是怎么炼成的 - GPT社会化的过程 (李宏毅)](https://www.bilibili.com/video/BV1U84y167i3)

![](/public/upload/machine/gpt_to_chatgpt.jpg)

1. 学习文字接龙
    1. 输入“你好”，gpt 可以接“美”（你好美）。当然，“你好” 后面也可以是 “嘛”，也可以是“高”。gpt 的输出是随机的，哪个在它学习的素材里出现的概率高就显示哪个。
    2. 文字接龙的学习是不需要人工标注的，网络上的每一段文字都可以教机器做文字接龙。
    2. 文字接龙有什么用？可以回答问题。你问gpt “台湾最高的山是那一座？”，gpt 可能接“玉”，你输入“台湾最高的山是那一座？玉”，它可能会接“山”。我们就知道答案是“玉山”。
    4. 你问gpt “台湾最高的山是那一座？”，gpt 也能有其它回答，比如gpt学习的素材里有一个选择题：“台湾最高的山是那一座？A 玉山；B 雪山”，也可能“台湾最高的山是那一座？谁能告诉我答案？”。这个时候chatgpt 是一个自由自在的孩子。
2. 人类老师引导文字接龙方向。gpt ==> chatgpt
    1. gpt 在网络上看到过各式各样奇怪的东西，可能产生各式各样奇怪的回答，不知道哪些是人类希望它回答的。
    1. 找人来思考想问gpt的问题，并人工提供正确答案。针对“台湾最高的山是那一座？”，gpt 有多个回答，然后由人来告诉它（标注） 人类想要的回答是“玉山”，人类只需要告诉机器，哪个答案是比较好的，哪个答案是比较差。
3. 模仿人类老师的喜好
    1. 训练一个模仿人类老师的模型，teacher model： 给一个问题，再给一个gpt 提供的答案，它负责输出一个分数。它学习的目标就是模仿人类的标准/偏好。
4. 用增强式学习向模拟老师学习
    1. 问gpt “世界最高的山是哪座？”，gpt 接了一句“世界上最深的海又在哪里？”，这是 对gpt 是一个合理的回答，但不是人类想要的。
    3. 把上述问题 和答案丢给 teacher model（已经学会了人类的偏好），teacher model 给一个score，也就是增强式学习（reinforcement learning）的reward。
    4. 接下来就是根据 reward 调整gpt的参数，目标是获取更高的score。经过reinforcement learning之后，就是chatgpt 了。

使用者感受chatgpt 的功能是：生成式学习。但chatgpt 实际做的事儿是 分类：给一句话，从所有的word 中选出一个word输出。 

[chatgpt 带来的研究问题](https://www.bilibili.com/video/BV1jA411274Q?p=3)
1. 如何精准提出需求（Prompting）？它不是一个聊天机器人，它只是根据你说的话生成后面的话，得调教它，靠人工还是靠系统？
2. 如果更正错误？chatgpt的预训练资料只有到2021年。
3. 侦测一段话是不是AI 生成的。
4. 泄露机密。

## 其它

[投身LLM，要从本质上想明白的三个问题：未来是什么，哪些机会更好，我们要怎么准备](https://mp.weixin.qq.com/s/wWWGP7-lReON_b5-Pm5plg) 好文。

神经网络训练目标是使预测损失最小化，在各个参数展开的空间内找到最优的点，如果从头开始找，会比较慢；但是从之前已经训练好的其他类似模型开始，**就相当于在最优点附近的点开始**，自然收敛的速度和效果会比从零训练好得多。取预训练好的网络的部分结构和权重，与自己新增的网络部分一起训练，称为微调（fine-tune）。

BERT由两阶段构成，每个阶段有自己的特点和目标。第一个阶段是预训练阶段，第二个阶段是Fine-Tuning阶段。预训练阶段用大量无监督的文本通过自监督方式进行训练，把文本包含的语言知识以参数形式编码到Transformer中，Fine-Tuning一般是有监督的，数据量比较小，在模型结构上做分类任务以解决当前任务。第一阶段跟第二阶段怎么连接起来的？在预训练阶段Transformer学到了很多初始化的知识，第二阶段就把初始化网络学到的语言知识拿来用，Fine-Tuning引入新的特征解决你的问题。所以，为什么BERT效果这么好？为什么以前的模型效果没有BERT好？因为，第一阶段编码了文本中大量的语言学知识，在Bert之前，没有用那么多的文本数据，而且是无监督的方式。那么我们关心的是：BERT里的Transformer到底学到了什么？比传统模型多学了什么知识？这是关键。如果归纳一下目前的研究结论的话，大致概述一下：BERT训练好之后，低层Transformer主要学习自然语言表层的特征，中层学习编码句法信息，高层编码了NLP的语义特征。很多实验都已证明这一结论。


