---

layout: post
title: 如何应用LLM
category: 架构
tags: MachineLearning
keywords: llm chatgpt gpt bert

---

## 简介

* TOC
{:toc}

早期的深度模型专模专用，严重依赖有监督学习，这就需要大量的任务相关的人工标注数据，代价昂贵。天下苦标注久矣，如果能够有一个与具体任务无关的大模型，只要利用任务相关的少量数据微调，就能够大杀四方，岂不美哉。

## LLM表现出了惊人的语义理解和生成能力，这种能力如何赋能企业核心业务？


[自建行业大模型的思考](https://zhuanlan.zhihu.com/p/625186095)

应用场景
1. 企业内部应用：主要作用于帮助企业员工更好的完成本职工作，提升工作效率
2. 企业对外业务：将LLM封装成一个产品，销售给客户

是否需要构建行业大模型？
1. 通用 LLM会不会大力出奇迹，在没有经过专业领域数据训练的条件下就可以很好的完成专业领域任务？BloombergGPT的论文中实现表明：基于专业领域语料训练的大模型，在领域内的理解要超过通用大模型；
2. 能否通过为Prompt填充领域知识的方式，让LLM具备解决专业领域任务的能力？可以，角色扮演就是一个例子
    1. 优点：灵活多变，可以适应各种场景，几乎没有训练成本，所有的行业知识通过prompt注入
    2. 缺点：大量领域知识会限制多轮对话或者prompt构造，模型输入有长度限制，如果加入了较多领域知识，就没有空间留给理会对话以及prompt；对于专业词汇的理解可能不准确
3.  能否通过检索的方式，从本地知识库中查找出符合要求的答案返回？可以，例如：插件、AutoGPT
    1. 优点：没有训练成本；返回结果完全可控，即知识库内容
    2. 缺点：大模型+检索的方式分成两步走，性能会较慢；回答内容单一，缺乏泛化性；对于专业词汇的理解可能不准确

自建 or 购买
1. 购买。数据安全风险；缺乏商业护城河；不够灵活：新的需求和模型效果提升可能需要重新签订协议购买
2. 自建。训练成本较高：数据成本、算力成本、试错成本；模型效果难以保证；对于模型训练算法工程师有较高要求

自建大语言模型可能遇到的问题
1. 容易解决的问题
    1. 模型训练方案容易实现，现阶段开源LLM都有完善的训练和推理流程；
    2. 效果优异的基座模型，虽然中文开源LLM效果还没有英文的那么多、那么好，但是也已经达到了够用的水平；
2. 不容易解决的问题
    1. 明确的LLM需求
        1. 高级收益需求：某个功能以前无法实现，LLM可以助力实现，并为公司带来较高收益；
        2. 次级收益需求：某个已经实现的功能，LLM可以对其优化，降本增效；
    2. 训练数据难以构建，训练数据应该具备一下特征：训练大模型需要巨量数据；涵盖多种问题，同一问题还应该有多种表达方式；数据应该尽量准确，不能含有有毒数据；要包含大量通用文本语料，在二次训练过程中会有灾难性遗忘问题，需要通用语料保持LLM通用语言能力；要包含本公司业务领域的文本语料，提升LLM对于行业数据理解能力；最好包含多轮对话语料，使模型具备连续对话的能力；最好能有同一个问题的不同得分的回答，实现RLHF的训练
    3. 部署成本高昂：LLM模型参数量巨大，一般自建模型参数量最小是7B，需要占用大量显存；即使使用量化技术，推理服务QPS也不会很高；使用量化技术会让模型虽然会提升性能，降低显存占用，但是会极大损害效果；较长的sequence length会占用大量的显存，增加计算消耗（时间复杂度是sequence length的平方）；高性能GPU服务器成本较高
    4. 大语言模型生成结果不可控：生成模型普遍存在生成结果不可控的问题；需要尝试不同的prompt才能得到令人满意的结果

