---

layout: post
title: LLM外挂知识库
category: 技术
tags: MachineLearning
keywords: llm retrieval

---

* TOC
{:toc}

## 简介

RAG(Retrieval-Augmented Generation)，LLM相比传统算法最重要的就是zero shot能力，最重要的是从海量的文档中找到和问题相关的片段。找到后使用LLM生成回答其实很鸡肋，尤其是针对技术问题的回答，不需要LLM 去造答案，LLM 就帮我找到相关文档就行了。

## 引入外部知识 的几个示例

大语言模型的原理，就是利用训练样本里面出现的文本的前后关系，通过前面的文本对接下来出现的文本进行概率预测。如果类似的前后文本出现得越多，那么这个概率在训练过程里会收敛到少数正确答案上，回答就准确。如果这样的文本很少，那么训练过程里就会有一定的随机性，对应的答案就容易似是而非。

LLM 擅长于一般的语言理解与推理，而不是某个具体的知识点。如何为ChatGPT/LLM大语言模型添加额外知识？
1. 通过fine-tuning来和新知识及私有数据进行对话，OpenAI 模型微调的过程，并不复杂。你只需要把数据提供给 OpenAI 就好了，对应的整个微调的过程是在云端的“黑盒子”里进行的。需要提供的数据格式是一个文本文件，每一行都是一个 Prompt，以及对应这个 Prompt 的 Completion 接口会生成的内容。
    ```
    {"prompt": "<prompt text>", "completion": "<ideal generated text>"}
    {"prompt": "<prompt text>", "completion": "<ideal generated text>"}
    {"prompt": "<prompt text>", "completion": "<ideal generated text>"}
    ...
    ```
    有了准备好的数据，我们只要再通过 subprocess 调用 OpenAI 的命令行工具，来提交微调的指令就可以了。
    ```
    subprocess.run('openai api fine_tunes.create --training_file data/prepared_data_prepared.jsonl --model curie --suffix "ultraman"'.split())
    ```
    微调模型还有一个能力，不断收集新的数据，不断在前一个微调模型的基础之上继续微调我们的模型。
2. 通过word embeddings + pinecone数据库来搭建自己私有知识库。 chatgpt预训练完成后，会生成一个embeddings向量字典，比如我们可以将我们的私有知识库各个章节通过openai的相关api获取到对应的embeddings，然后将这些embeddings保存到向量数据库（比如 Facebook 开源的 Faiss库、Pinecone 和 Weaviate），当用户要对某个领域后者问题进行语义查询时，则将用户的输入同样通过openai的相关api来获取相应的embeddings向量，然后再和向量数据库pinecone中的我们的私有知识库类型做**语义相似度查询**，然后返回给用户。PS： 内容向量化
    1. 比如判断某一段文本 是积极还是消极，向chatgpt 查询目标文本的向量，然后计算其与“积极” “消极” 两个词 embedding 向量的“距离”，谁更近，说明这段文本更偏向于积极或消极。
    2. 过几天openAI的模型版本升级了，这些保存的embedding会失效吗？特定模型也有带日期的快照版本，选取那些快照版本就好了。
    3. 向量是基于大模型生成的，因此对两段文本向量相似度计算必须基于同一个模型，不同的模型算出来的向量之间是没有任何关系的，甚至连维数都不一样。不过你可以把基于A 模型来算向量相似度进行检索把文本找出来，然后把找到的文本喂给B模型来回答问题。 
3. 通过langchain这个chatgpt编程框架来给chatgpt赋能。 langchain可以将不同的工具模块和chatgpt给链接（chain）起来。
4. chatgpt 插件，比如有一个提供酒旅租车信息的插件
    ![](/public/upload/machine/chatgpt_plugins.jpg)

比如针对问题：鲁迅先生去日本学习医学的老师是谁。因为 LLM（大语言模型）对上下文长度的限制，你不能将《藤野先生》整体作为提示语然后问“鲁迅在日本的医学老师是谁？”。 先通过搜索的方式，找到和询问的问题最相关的语料。可以用传统的基于关键词搜索的技术。也可以先分块存到向量数据库中（向量和文本块之间的关系），使用 Embedding 的相似度进行语义搜索的技术。然后，我们将和问题语义最接近的前几条内容，作为提示语的一部分给到 AI（**使用检索结果作为 LLM 的 Prompt**）。然后请 AI 参考这些内容，再来回答这个问题。

![](/public/upload/machine/use_llm_with_search.jpg)
![](/public/upload/machine/llm_with_embedding.jpg)

这也是利用大语言模型的一个常见模式（这个模式实在太过常用了，所以有人为它写了一个开源 Python 包，叫做 llama-index）。因为**大语言模型其实内含了两种能力**。PS：有点像推荐的粗排和精排，纯向量化的召回在一些Benchmark上表现还不如关键字搜索。
1. 海量的语料中，本身已经包含了的知识信息。比如，我们前面问 AI 鱼香肉丝的做法，它能回答上来就是因为语料里已经有了充足的相关知识。我们一般称之为“世界知识”。
2. 根据你输入的内容，理解和推理的能力。这个能力，不需要训练语料里有一样的内容。而是大语言模型本身有“思维能力”，能够进行阅读理解。这个过程里，“知识”不是模型本身提供的，而是我们找出来，临时提供给模型的。如果不提供这个上下文，再问一次模型相同的问题，它还是答不上来的。

**Embedding 生成向量使用的模型  跟最后prompt 调用的模型可以不是同一个**，因此有的向量数据库也包含Segment 和 Embedding，**通过自然语言就能直接和向量数据库交互**。也就是说，我们可以直接把文档扔给数据库，大段文本的切分，以及文本向量化，向量数据库 会帮我们处理。我们也可以直接把问题扔给数据库，请他来查询相似度较高的文本块，问题向量化以及检索的细节，向量数据库会帮我们处理。

[基于大语言模型构建知识问答系统](https://zhuanlan.zhihu.com/p/627655485) 
1. 传统搜索系统基于关键字匹配，在面向：游戏攻略、技术图谱、知识库等业务场景时，缺少对用户问题理解和答案二次处理能力。
2. 领域知识不在预训练的数据集中，比如：
    1. 较新的内容。同一个知识点不断变更：修改、删除、添加。如何反馈当前最新的最全面的知识。比如对于 ChatGpt 而言，训练数据全部来自于 2021.09 之前。
    2. 未公开的、未联网的内容。
3. 基于 LLM 搭建问答系统的解决方案有以下几种：
    1. Fine-Tuning
    2. 基于 Prompt Engineering，比如 Few-Shot方式。**将特定领域的知识作为输入消息提供给模型**。类似于短期记忆，容量有限但是清晰。举个例子给 ChatGPT 发送请求，将特定的知识放在请求中，让 ChatGPT 对消息中蕴含的知识进行分析，并返回处理结果。
    3. 与普通搜索结合，使用基础模型对搜索结果加工。在做问答时的方式就是把 query 转换成向量，然后在文档向量库中做相似度搜索。
        ![](/public/upload/machine/llm_with_knowledge_base.jpg)
    4. 用户输入query之后，首先先从知识库搜索到结果，然后基于搜索到的结果进行解析构造，生成新的prompt，然后调用LLM，LLM根据输入的prompt自行进行知识库的检索与plugins的调用
        ![](/public/upload/machine/use_llm_with_prompt.jpg)

## 难点

上下文注入，即不修改LLM，专注于提示本身，并将相关上下文注入到提示中，让模型参考这个提示进行作答，但是其问题在于如何为提示提供正确的信息。目前我们所能看到的就是**相关性召回**，其有个假设，即问题的答案在召回的最相似的文档里。因此，如何处理非结构内容跟进行文本拆分十分重要，尤其是针对复杂文本的场景。
1. 经常遇到一些复杂文档的情况，这些文档中可能有表格，有图片，有单双栏等情况。尤其是对于一些扫描版本的文档时候，则需要将文档转换成可以编辑的文档，这就变成了版面还原的问题。具体的，可以利用ppstructrue进行文档版面分析，在具体实现路线上，图像首先经过版面分析模型，将图像划分为文本、表格、图像等不同区域，随后对这些区域分别进行识别。
2. 如何将文档拆分为文本片段。
    1. 如何保证文档切片不会造成相关内容的丢失？一般而言，文本分割如果按照字符长度进行分割，这是最简单的方式，但会带来很多问题。例如，如果文本是一段代码，一个函数被分割到两段之后就成了没有意义的字符。因此，我们也通常会使用特定的分隔符进行切分，如句号，换行符，问号等。可以使用专门的模型去切分文本，尽量保证一个chunk的语义是完整的，且长度不会超过限制。
    2. 文档切片的大小如何控制？ 太小则 容易造成信息丢失，太大则不利于向量检索命中。
3. 文档召回过程中如何保证召回内容跟问题是相关的？ embedding 模型 可能从未见过你文档的内容，也许你的文档的相似词也没有经过训练。所以不能保证召回的内容就非常准确，不准确则导致LLM回答容易产生幻觉（简而言之就是胡说八道）。
    1. 将用户问题采用多个不同的视角去提问，然后 LLM 会得出最终结果。大多数人在问问题的过程中，如果不懂 prompt 工程，往往不专业，要么问题过于简单化，要么有歧义，意图不明显。那么向量搜索也是不准确的，导致LLM回答的效果不好。所以需要 LLM 进行问题的修正和多方位解读。MultiQueryRetriever 
    3. 对一篇文档/chunk生成知识点、问题、短摘要，当根据query 进行匹配时，可能先匹配到知识点、问题、短摘要，再找到原始chunk。MultiVectorRetriever/ParentDocumentRetriever 
    4. 同时使用了es 和向量召回，比如es 召回了10篇chunk，向量召回了10篇chunk，使用rerank model/ LongContextReorder （LLM 对位置是相对比较敏感的，得分好的放在首或尾，LLM会重点关注） 从中选出topk chunk 传给LLM生成回答。 
4. 用户的问题完全不相关怎么处理？
5. 文档向量搜索完全不准（大概率未训练，内容属于专业知识）如何弥补？

## 优化

1. 缓存
    1. LangChain 提供了 CacheBackedEmbeddings ， 可以提高 embedings 的二次加载和解析的效率，首次正常速度，后续有一个 3倍效率的提升。